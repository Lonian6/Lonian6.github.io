---
title:          "Training-Efficient Text-to-Music Generation with State-Space Modeling"
date:           2026-01-21
selected:       true
pub:            "Under review"
# pub_ab:         ""
pub_date:       "2026"
abstract: >-
    We propose a hybrid Auto-Regressive (AR) and Non-Auto-Regressive (NAR) architecture for coarse-to-fine music generation. Our approach employs a State-Space Model (SSM) as the language model to generate coarse tokens, followed by a pre-trained diffusion model for fine-grained refinement. By leveraging the linear scaling of SSMs, our model achieves significantly higher training efficiency compared to traditional Transformer-based architectures.
cover: /assets/images/covers/cover_2026_Training-Efficient Text-to-Music Generation with State-Space Modeling.png
authors:
- Wei-Jaw Lee
- Fang-Chih Hsieh
- Xuanjun Chen
- Fang-Duo Tsai
- Yi-Hsuan Yang
links:
    Web: https://lonian6.github.io/ssmttm/
    Paper: https://arxiv.org/abs/2601.14786
    Code: https://github.com/Lonian6/SSM-TTM
---